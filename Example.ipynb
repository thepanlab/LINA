{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/adbadre/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import sys  \n",
    "sys.path.insert(0, './')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#Local Libraries\n",
    "from LINA import LINA_Network\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_examples,n_features):\n",
    "        data = np.random.uniform(\n",
    "            -1, 1, size=(n_examples, n_features))\n",
    "        y = (-20*np.sin(2*data[:, 0]*data[:, 1]+data[:, 7])\n",
    "                                  + 2*np.abs(data[:, 2])\n",
    "                                  + 4*data[:, 3]*data[:, 4]\n",
    "                                  - np.exp(-data[:, 5])\n",
    "                                  ).reshape(-1, 1)\n",
    "        return data,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y=generate_data(10000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 339., 1209., 1184., 1082., 1012., 1034., 1111., 1184., 1309.,\n",
       "         536.]), array([-24.99001129, -20.10795431, -15.22589732, -10.34384033,\n",
       "         -5.46178335,  -0.57972636,   4.30233063,   9.18438762,\n",
       "         14.0664446 ,  18.94850159,  23.83055858]), <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b370b3aa0b8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare Training,Validation,Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=X[:X.shape[0]*80//100,:]\n",
    "x_test=X[X.shape[0]*80//100:,:]\n",
    "y=Y[:X.shape[0]*80//100,:]\n",
    "y_test=Y[X.shape[0]*80//100:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsquare(y_true,y_pred):\n",
    "    return 1-tf.keras.losses.MSE(y_true,y_pred)/tf.square(tf.keras.backend.std(y_true))\n",
    "def rmse(y_true,y_pred):\n",
    "    return tf.math.sqrt(tf.keras.losses.MSE(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/100\n",
      "6400/6400 [==============================] - 3s 503us/sample - loss: 103.1154 - loss_classic: 103.1079 - rmse: 8.6457 - rsquare: 0.3609 - val_loss: 149155.0656 - val_loss_classic: 149154.7344 - val_rmse: 288.7096 - val_rsquare: -914.8935\n",
      "Epoch 2/100\n",
      "6400/6400 [==============================] - 1s 146us/sample - loss: 20.5901 - loss_classic: 20.5780 - rmse: 3.4875 - rsquare: 0.8720 - val_loss: 72588.2084 - val_loss_classic: 72587.9375 - val_rmse: 203.9247 - val_rsquare: -444.2130\n",
      "Epoch 3/100\n",
      "6400/6400 [==============================] - 1s 98us/sample - loss: 8.4381 - loss_classic: 8.4252 - rmse: 2.1790 - rsquare: 0.9479 - val_loss: 29784.7105 - val_loss_classic: 29784.5391 - val_rmse: 123.9814 - val_rsquare: -181.4955\n",
      "Epoch 4/100\n",
      "6400/6400 [==============================] - 0s 55us/sample - loss: 4.7179 - loss_classic: 4.7050 - rmse: 1.5717 - rsquare: 0.9707 - val_loss: 8050.1547 - val_loss_classic: 8050.0649 - val_rmse: 63.5083 - val_rsquare: -48.4893\n",
      "Epoch 5/100\n",
      "6400/6400 [==============================] - 1s 90us/sample - loss: 3.2913 - loss_classic: 3.2784 - rmse: 1.3176 - rsquare: 0.9796 - val_loss: 2992.1663 - val_loss_classic: 2992.1108 - val_rmse: 40.7547 - val_rsquare: -17.4053\n",
      "Epoch 6/100\n",
      "6400/6400 [==============================] - 0s 47us/sample - loss: 2.1483 - loss_classic: 2.1354 - rmse: 1.0761 - rsquare: 0.9867 - val_loss: 1430.8554 - val_loss_classic: 1430.8152 - val_rmse: 29.2152 - val_rsquare: -7.7525\n",
      "Epoch 7/100\n",
      "6400/6400 [==============================] - 0s 50us/sample - loss: 1.6271 - loss_classic: 1.6143 - rmse: 0.9460 - rsquare: 0.9900 - val_loss: 766.4488 - val_loss_classic: 766.4182 - val_rmse: 22.0880 - val_rsquare: -3.7161\n",
      "Epoch 8/100\n",
      "6400/6400 [==============================] - 3s 423us/sample - loss: 1.9988 - loss_classic: 1.9861 - rmse: 0.9991 - rsquare: 0.9877 - val_loss: 501.3502 - val_loss_classic: 501.3239 - val_rmse: 17.9512 - val_rsquare: -2.0782\n",
      "Epoch 9/100\n",
      "6400/6400 [==============================] - 1s 108us/sample - loss: 1.8183 - loss_classic: 1.8056 - rmse: 0.9720 - rsquare: 0.9887 - val_loss: 368.7060 - val_loss_classic: 368.6830 - val_rmse: 15.6519 - val_rsquare: -1.2588\n",
      "Epoch 10/100\n",
      "6400/6400 [==============================] - 1s 126us/sample - loss: 1.7101 - loss_classic: 1.6974 - rmse: 0.9389 - rsquare: 0.9895 - val_loss: 272.8287 - val_loss_classic: 272.8076 - val_rmse: 13.4752 - val_rsquare: -0.6748\n",
      "Epoch 11/100\n",
      "6400/6400 [==============================] - 1s 92us/sample - loss: 1.7665 - loss_classic: 1.7539 - rmse: 0.9714 - rsquare: 0.9892 - val_loss: 194.8323 - val_loss_classic: 194.8125 - val_rmse: 11.4462 - val_rsquare: -0.1942\n",
      "Epoch 12/100\n",
      "6400/6400 [==============================] - 1s 98us/sample - loss: 1.5205 - loss_classic: 1.5080 - rmse: 0.8779 - rsquare: 0.9906 - val_loss: 199.7186 - val_loss_classic: 199.6996 - val_rmse: 11.4138 - val_rsquare: -0.2251\n",
      "Epoch 13/100\n",
      "6400/6400 [==============================] - 1s 119us/sample - loss: 1.5801 - loss_classic: 1.5676 - rmse: 0.9097 - rsquare: 0.9903 - val_loss: 158.4159 - val_loss_classic: 158.3974 - val_rmse: 10.2981 - val_rsquare: 0.0282\n",
      "Epoch 14/100\n",
      "6400/6400 [==============================] - 1s 79us/sample - loss: 1.1594 - loss_classic: 1.1469 - rmse: 0.7858 - rsquare: 0.9928 - val_loss: 111.3847 - val_loss_classic: 111.3671 - val_rmse: 8.7457 - val_rsquare: 0.3168\n",
      "Epoch 15/100\n",
      "6400/6400 [==============================] - 0s 37us/sample - loss: 1.1354 - loss_classic: 1.1229 - rmse: 0.7434 - rsquare: 0.9930 - val_loss: 94.6169 - val_loss_classic: 94.6000 - val_rmse: 7.9240 - val_rsquare: 0.4205\n",
      "Epoch 16/100\n",
      "6400/6400 [==============================] - 0s 51us/sample - loss: 0.9809 - loss_classic: 0.9685 - rmse: 0.7236 - rsquare: 0.9940 - val_loss: 67.5689 - val_loss_classic: 67.5528 - val_rmse: 6.6712 - val_rsquare: 0.5838\n",
      "Epoch 17/100\n",
      "6400/6400 [==============================] - 1s 86us/sample - loss: 1.1594 - loss_classic: 1.1470 - rmse: 0.7703 - rsquare: 0.9929 - val_loss: 56.6268 - val_loss_classic: 56.6112 - val_rmse: 6.1932 - val_rsquare: 0.6520\n",
      "Epoch 18/100\n",
      "6400/6400 [==============================] - 0s 44us/sample - loss: 1.2135 - loss_classic: 1.2012 - rmse: 0.8041 - rsquare: 0.9925 - val_loss: 49.6750 - val_loss_classic: 49.6596 - val_rmse: 5.6812 - val_rsquare: 0.6951\n",
      "Epoch 19/100\n",
      "6400/6400 [==============================] - 1s 165us/sample - loss: 0.9472 - loss_classic: 0.9349 - rmse: 0.7148 - rsquare: 0.9942 - val_loss: 40.8072 - val_loss_classic: 40.7919 - val_rmse: 5.2241 - val_rsquare: 0.7497\n",
      "Epoch 20/100\n",
      "6400/6400 [==============================] - 1s 116us/sample - loss: 0.7447 - loss_classic: 0.7323 - rmse: 0.6200 - rsquare: 0.9955 - val_loss: 28.7568 - val_loss_classic: 28.7421 - val_rmse: 4.3073 - val_rsquare: 0.8233\n",
      "Epoch 21/100\n",
      "6400/6400 [==============================] - 0s 46us/sample - loss: 0.9077 - loss_classic: 0.8954 - rmse: 0.6972 - rsquare: 0.9944 - val_loss: 29.1325 - val_loss_classic: 29.1180 - val_rmse: 4.3886 - val_rsquare: 0.8217\n",
      "Epoch 22/100\n",
      "6400/6400 [==============================] - 1s 110us/sample - loss: 1.1134 - loss_classic: 1.1011 - rmse: 0.7587 - rsquare: 0.9931 - val_loss: 21.6091 - val_loss_classic: 21.5950 - val_rmse: 3.6587 - val_rsquare: 0.8674\n",
      "Epoch 23/100\n",
      "6400/6400 [==============================] - 1s 145us/sample - loss: 1.3211 - loss_classic: 1.3089 - rmse: 0.8001 - rsquare: 0.9919 - val_loss: 24.2199 - val_loss_classic: 24.2059 - val_rmse: 3.9914 - val_rsquare: 0.8513\n",
      "Epoch 24/100\n",
      "6400/6400 [==============================] - 1s 195us/sample - loss: 0.9376 - loss_classic: 0.9254 - rmse: 0.6911 - rsquare: 0.9943 - val_loss: 20.9424 - val_loss_classic: 20.9285 - val_rmse: 3.4885 - val_rsquare: 0.8720\n",
      "Epoch 25/100\n",
      "6400/6400 [==============================] - 1s 121us/sample - loss: 1.1494 - loss_classic: 1.1371 - rmse: 0.7765 - rsquare: 0.9929 - val_loss: 26.3554 - val_loss_classic: 26.3413 - val_rmse: 4.1015 - val_rsquare: 0.8383\n",
      "Epoch 26/100\n",
      "6400/6400 [==============================] - 0s 75us/sample - loss: 1.3239 - loss_classic: 1.3118 - rmse: 0.8051 - rsquare: 0.9918 - val_loss: 21.3865 - val_loss_classic: 21.3726 - val_rmse: 3.7007 - val_rsquare: 0.8683\n",
      "Epoch 27/100\n",
      "6400/6400 [==============================] - 0s 56us/sample - loss: 1.1967 - loss_classic: 1.1845 - rmse: 0.7743 - rsquare: 0.9926 - val_loss: 26.4528 - val_loss_classic: 26.4389 - val_rmse: 4.1481 - val_rsquare: 0.8378\n",
      "Epoch 28/100\n",
      "6400/6400 [==============================] - 0s 48us/sample - loss: 1.1821 - loss_classic: 1.1700 - rmse: 0.7925 - rsquare: 0.9927 - val_loss: 27.2077 - val_loss_classic: 27.1937 - val_rmse: 4.1710 - val_rsquare: 0.8334\n",
      "Epoch 29/100\n",
      "6400/6400 [==============================] - 1s 104us/sample - loss: 1.0873 - loss_classic: 1.0752 - rmse: 0.7208 - rsquare: 0.9933 - val_loss: 16.0984 - val_loss_classic: 16.0847 - val_rmse: 3.2482 - val_rsquare: 0.9015\n",
      "Epoch 30/100\n",
      "6400/6400 [==============================] - 0s 62us/sample - loss: 1.2147 - loss_classic: 1.2026 - rmse: 0.7905 - rsquare: 0.9926 - val_loss: 26.5523 - val_loss_classic: 26.5386 - val_rmse: 4.1265 - val_rsquare: 0.8374\n",
      "Epoch 31/100\n",
      "6400/6400 [==============================] - 0s 32us/sample - loss: 1.0502 - loss_classic: 1.0381 - rmse: 0.7192 - rsquare: 0.9936 - val_loss: 26.0815 - val_loss_classic: 26.0676 - val_rmse: 4.1395 - val_rsquare: 0.8400\n",
      "Epoch 32/100\n",
      "6400/6400 [==============================] - 0s 48us/sample - loss: 1.0368 - loss_classic: 1.0248 - rmse: 0.7298 - rsquare: 0.9936 - val_loss: 25.9712 - val_loss_classic: 25.9574 - val_rmse: 4.0198 - val_rsquare: 0.8396\n",
      "Epoch 33/100\n",
      "6400/6400 [==============================] - 0s 25us/sample - loss: 0.9018 - loss_classic: 0.8898 - rmse: 0.6782 - rsquare: 0.9945 - val_loss: 18.0244 - val_loss_classic: 18.0110 - val_rmse: 3.1868 - val_rsquare: 0.8895\n",
      "Epoch 34/100\n",
      "6400/6400 [==============================] - 0s 28us/sample - loss: 1.2338 - loss_classic: 1.2219 - rmse: 0.8039 - rsquare: 0.9924 - val_loss: 20.5857 - val_loss_classic: 20.5721 - val_rmse: 3.6064 - val_rsquare: 0.8738\n",
      "Epoch 35/100\n",
      "6400/6400 [==============================] - 0s 30us/sample - loss: 0.8220 - loss_classic: 0.8100 - rmse: 0.6584 - rsquare: 0.9949 - val_loss: 20.3344 - val_loss_classic: 20.3209 - val_rmse: 3.5228 - val_rsquare: 0.8746\n",
      "Epoch 36/100\n",
      "6400/6400 [==============================] - 0s 28us/sample - loss: 0.8474 - loss_classic: 0.8355 - rmse: 0.6613 - rsquare: 0.9948 - val_loss: 20.5537 - val_loss_classic: 20.5401 - val_rmse: 3.5507 - val_rsquare: 0.8739\n",
      "Epoch 37/100\n",
      "6400/6400 [==============================] - 0s 42us/sample - loss: 0.7767 - loss_classic: 0.7647 - rmse: 0.6228 - rsquare: 0.9953 - val_loss: 14.2099 - val_loss_classic: 14.1966 - val_rmse: 2.9028 - val_rsquare: 0.9130\n",
      "Epoch 38/100\n",
      "6400/6400 [==============================] - 2s 243us/sample - loss: 0.8734 - loss_classic: 0.8614 - rmse: 0.6856 - rsquare: 0.9946 - val_loss: 10.8469 - val_loss_classic: 10.8337 - val_rmse: 2.5569 - val_rsquare: 0.9335\n",
      "Epoch 39/100\n",
      "6400/6400 [==============================] - 0s 37us/sample - loss: 0.9229 - loss_classic: 0.9109 - rmse: 0.6950 - rsquare: 0.9943 - val_loss: 12.2215 - val_loss_classic: 12.2083 - val_rmse: 2.7220 - val_rsquare: 0.9252\n",
      "Epoch 40/100\n",
      "6400/6400 [==============================] - 0s 30us/sample - loss: 1.0031 - loss_classic: 0.9911 - rmse: 0.7235 - rsquare: 0.9939 - val_loss: 13.4608 - val_loss_classic: 13.4475 - val_rmse: 2.7999 - val_rsquare: 0.9174\n",
      "Epoch 41/100\n",
      "6400/6400 [==============================] - 0s 27us/sample - loss: 1.0464 - loss_classic: 1.0344 - rmse: 0.7373 - rsquare: 0.9936 - val_loss: 12.6405 - val_loss_classic: 12.6273 - val_rmse: 2.7291 - val_rsquare: 0.9225\n",
      "Epoch 42/100\n",
      "6400/6400 [==============================] - 0s 59us/sample - loss: 0.9086 - loss_classic: 0.8966 - rmse: 0.6923 - rsquare: 0.9944 - val_loss: 14.0254 - val_loss_classic: 14.0122 - val_rmse: 2.9243 - val_rsquare: 0.9140\n",
      "Epoch 43/100\n",
      "6400/6400 [==============================] - 0s 30us/sample - loss: 1.0130 - loss_classic: 1.0011 - rmse: 0.7243 - rsquare: 0.9938 - val_loss: 16.0730 - val_loss_classic: 16.0599 - val_rmse: 3.1450 - val_rsquare: 0.9010\n",
      "Epoch 44/100\n",
      "6400/6400 [==============================] - 0s 30us/sample - loss: 1.0242 - loss_classic: 1.0124 - rmse: 0.7152 - rsquare: 0.9937 - val_loss: 14.1161 - val_loss_classic: 14.1031 - val_rmse: 2.8557 - val_rsquare: 0.9134\n",
      "Epoch 45/100\n",
      "6400/6400 [==============================] - 0s 41us/sample - loss: 0.9527 - loss_classic: 0.9409 - rmse: 0.7134 - rsquare: 0.9942 - val_loss: 15.7335 - val_loss_classic: 15.7202 - val_rmse: 3.1721 - val_rsquare: 0.9034\n",
      "Epoch 46/100\n",
      "6400/6400 [==============================] - 0s 38us/sample - loss: 0.7036 - loss_classic: 0.6917 - rmse: 0.6017 - rsquare: 0.9957 - val_loss: 14.3209 - val_loss_classic: 14.3078 - val_rmse: 2.9732 - val_rsquare: 0.9128\n",
      "Epoch 47/100\n",
      "6400/6400 [==============================] - 0s 40us/sample - loss: 0.9995 - loss_classic: 0.9877 - rmse: 0.7153 - rsquare: 0.9939 - val_loss: 11.3027 - val_loss_classic: 11.2898 - val_rmse: 2.5495 - val_rsquare: 0.9309\n",
      "Epoch 48/100\n",
      "6400/6400 [==============================] - 0s 66us/sample - loss: 0.9560 - loss_classic: 0.9442 - rmse: 0.7188 - rsquare: 0.9941 - val_loss: 9.5693 - val_loss_classic: 9.5561 - val_rmse: 2.4700 - val_rsquare: 0.9415\n",
      "Epoch 49/100\n",
      "6400/6400 [==============================] - 1s 106us/sample - loss: 0.8604 - loss_classic: 0.8485 - rmse: 0.6573 - rsquare: 0.9947 - val_loss: 7.9186 - val_loss_classic: 7.9057 - val_rmse: 2.1796 - val_rsquare: 0.9515\n",
      "Epoch 50/100\n",
      "6400/6400 [==============================] - 0s 63us/sample - loss: 0.9651 - loss_classic: 0.9533 - rmse: 0.7274 - rsquare: 0.9941 - val_loss: 11.9394 - val_loss_classic: 11.9262 - val_rmse: 2.8065 - val_rsquare: 0.9271\n",
      "Epoch 51/100\n",
      "6400/6400 [==============================] - 0s 35us/sample - loss: 0.9693 - loss_classic: 0.9575 - rmse: 0.7234 - rsquare: 0.9940 - val_loss: 10.3711 - val_loss_classic: 10.3580 - val_rmse: 2.5987 - val_rsquare: 0.9365\n",
      "Epoch 52/100\n",
      "6400/6400 [==============================] - 0s 46us/sample - loss: 0.9207 - loss_classic: 0.9090 - rmse: 0.7076 - rsquare: 0.9944 - val_loss: 10.8685 - val_loss_classic: 10.8555 - val_rmse: 2.5318 - val_rsquare: 0.9331\n",
      "Epoch 53/100\n",
      "6400/6400 [==============================] - 0s 39us/sample - loss: 1.0540 - loss_classic: 1.0421 - rmse: 0.7426 - rsquare: 0.9935 - val_loss: 10.4872 - val_loss_classic: 10.4740 - val_rmse: 2.5369 - val_rsquare: 0.9357\n",
      "Epoch 54/100\n",
      "6400/6400 [==============================] - 0s 54us/sample - loss: 0.7681 - loss_classic: 0.7563 - rmse: 0.6228 - rsquare: 0.9953 - val_loss: 7.4104 - val_loss_classic: 7.3974 - val_rmse: 2.0930 - val_rsquare: 0.9547\n",
      "Epoch 55/100\n",
      "6400/6400 [==============================] - 1s 81us/sample - loss: 0.8474 - loss_classic: 0.8356 - rmse: 0.6584 - rsquare: 0.9948 - val_loss: 9.4191 - val_loss_classic: 9.4061 - val_rmse: 2.3600 - val_rsquare: 0.9421\n",
      "Epoch 56/100\n",
      "6400/6400 [==============================] - 0s 62us/sample - loss: 0.7790 - loss_classic: 0.7673 - rmse: 0.6331 - rsquare: 0.9952 - val_loss: 9.5375 - val_loss_classic: 9.5244 - val_rmse: 2.4809 - val_rsquare: 0.9417\n",
      "Epoch 57/100\n",
      "6400/6400 [==============================] - 0s 54us/sample - loss: 0.9598 - loss_classic: 0.9480 - rmse: 0.7069 - rsquare: 0.9941 - val_loss: 8.2650 - val_loss_classic: 8.2521 - val_rmse: 2.2467 - val_rsquare: 0.9492\n",
      "Epoch 58/100\n",
      "6400/6400 [==============================] - 0s 31us/sample - loss: 0.9892 - loss_classic: 0.9774 - rmse: 0.7223 - rsquare: 0.9939 - val_loss: 11.9278 - val_loss_classic: 11.9148 - val_rmse: 2.5131 - val_rsquare: 0.9271\n",
      "Epoch 59/100\n",
      "6400/6400 [==============================] - 0s 43us/sample - loss: 0.9585 - loss_classic: 0.9467 - rmse: 0.7089 - rsquare: 0.9941 - val_loss: 11.1968 - val_loss_classic: 11.1839 - val_rmse: 2.4950 - val_rsquare: 0.9314\n",
      "Epoch 60/100\n",
      "6400/6400 [==============================] - 0s 28us/sample - loss: 0.9038 - loss_classic: 0.8920 - rmse: 0.6734 - rsquare: 0.9945 - val_loss: 11.1790 - val_loss_classic: 11.1662 - val_rmse: 2.4795 - val_rsquare: 0.9317\n",
      "Epoch 61/100\n",
      "6400/6400 [==============================] - 0s 33us/sample - loss: 0.8086 - loss_classic: 0.7970 - rmse: 0.6583 - rsquare: 0.9950 - val_loss: 11.7007 - val_loss_classic: 11.6878 - val_rmse: 2.6294 - val_rsquare: 0.9284\n",
      "Epoch 62/100\n",
      "6400/6400 [==============================] - 1s 126us/sample - loss: 0.7184 - loss_classic: 0.7067 - rmse: 0.6167 - rsquare: 0.9956 - val_loss: 7.2959 - val_loss_classic: 7.2829 - val_rmse: 2.1649 - val_rsquare: 0.9553\n",
      "Epoch 63/100\n",
      "6400/6400 [==============================] - 1s 107us/sample - loss: 0.7230 - loss_classic: 0.7112 - rmse: 0.5824 - rsquare: 0.9956 - val_loss: 6.0958 - val_loss_classic: 6.0830 - val_rmse: 2.0074 - val_rsquare: 0.9626\n",
      "Epoch 64/100\n",
      "6400/6400 [==============================] - 0s 77us/sample - loss: 0.8939 - loss_classic: 0.8823 - rmse: 0.6971 - rsquare: 0.9945 - val_loss: 7.2864 - val_loss_classic: 7.2735 - val_rmse: 2.2272 - val_rsquare: 0.9554\n",
      "Epoch 65/100\n",
      "6400/6400 [==============================] - 0s 74us/sample - loss: 0.7368 - loss_classic: 0.7251 - rmse: 0.6223 - rsquare: 0.9955 - val_loss: 6.1241 - val_loss_classic: 6.1113 - val_rmse: 1.9925 - val_rsquare: 0.9624\n",
      "Epoch 66/100\n",
      "6400/6400 [==============================] - 0s 33us/sample - loss: 0.7023 - loss_classic: 0.6906 - rmse: 0.6274 - rsquare: 0.9957 - val_loss: 6.8804 - val_loss_classic: 6.8677 - val_rmse: 2.0924 - val_rsquare: 0.9579\n",
      "Epoch 67/100\n",
      "6400/6400 [==============================] - 0s 34us/sample - loss: 1.1372 - loss_classic: 1.1256 - rmse: 0.7655 - rsquare: 0.9931 - val_loss: 8.9785 - val_loss_classic: 8.9657 - val_rmse: 2.3809 - val_rsquare: 0.9450\n",
      "Epoch 68/100\n",
      "6400/6400 [==============================] - 0s 37us/sample - loss: 1.1212 - loss_classic: 1.1095 - rmse: 0.7776 - rsquare: 0.9931 - val_loss: 12.8764 - val_loss_classic: 12.8632 - val_rmse: 2.8341 - val_rsquare: 0.9212\n",
      "Epoch 69/100\n",
      "6400/6400 [==============================] - 0s 37us/sample - loss: 0.8937 - loss_classic: 0.8820 - rmse: 0.6593 - rsquare: 0.9945 - val_loss: 10.2490 - val_loss_classic: 10.2361 - val_rmse: 2.5799 - val_rsquare: 0.9371\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400/6400 [==============================] - 0s 42us/sample - loss: 0.6772 - loss_classic: 0.6656 - rmse: 0.6066 - rsquare: 0.9959 - val_loss: 10.1388 - val_loss_classic: 10.1258 - val_rmse: 2.6352 - val_rsquare: 0.9379\n",
      "Epoch 71/100\n",
      "6400/6400 [==============================] - 0s 48us/sample - loss: 0.9520 - loss_classic: 0.9404 - rmse: 0.6985 - rsquare: 0.9942 - val_loss: 10.7065 - val_loss_classic: 10.6936 - val_rmse: 2.4961 - val_rsquare: 0.9345\n",
      "Epoch 72/100\n",
      "6400/6400 [==============================] - 0s 34us/sample - loss: 0.7754 - loss_classic: 0.7638 - rmse: 0.6501 - rsquare: 0.9953 - val_loss: 9.8276 - val_loss_classic: 9.8148 - val_rmse: 2.3401 - val_rsquare: 0.9397\n",
      "Epoch 73/100\n",
      "6400/6400 [==============================] - 1s 83us/sample - loss: 0.8542 - loss_classic: 0.8425 - rmse: 0.6606 - rsquare: 0.9948 - val_loss: 9.1434 - val_loss_classic: 9.1308 - val_rmse: 2.1456 - val_rsquare: 0.9440\n",
      "Epoch 74/100\n",
      "6400/6400 [==============================] - 0s 49us/sample - loss: 0.9156 - loss_classic: 0.9040 - rmse: 0.7012 - rsquare: 0.9944 - val_loss: 7.3180 - val_loss_classic: 7.3057 - val_rmse: 1.9618 - val_rsquare: 0.9552\n",
      "Epoch 75/100\n",
      "6400/6400 [==============================] - 2s 244us/sample - loss: 0.9886 - loss_classic: 0.9769 - rmse: 0.7225 - rsquare: 0.9939 - val_loss: 6.0271 - val_loss_classic: 6.0147 - val_rmse: 1.8947 - val_rsquare: 0.9631\n",
      "Epoch 76/100\n",
      "6400/6400 [==============================] - 0s 32us/sample - loss: 0.9077 - loss_classic: 0.8961 - rmse: 0.7024 - rsquare: 0.9944 - val_loss: 6.8430 - val_loss_classic: 6.8305 - val_rmse: 1.9560 - val_rsquare: 0.9581\n",
      "Epoch 77/100\n",
      "6400/6400 [==============================] - 0s 35us/sample - loss: 0.7261 - loss_classic: 0.7145 - rmse: 0.6234 - rsquare: 0.9956 - val_loss: 7.6552 - val_loss_classic: 7.6427 - val_rmse: 1.9945 - val_rsquare: 0.9531\n",
      "Epoch 78/100\n",
      "6400/6400 [==============================] - 0s 49us/sample - loss: 0.8875 - loss_classic: 0.8759 - rmse: 0.6868 - rsquare: 0.9945 - val_loss: 7.6269 - val_loss_classic: 7.6144 - val_rmse: 1.9950 - val_rsquare: 0.9534\n",
      "Epoch 79/100\n",
      "6400/6400 [==============================] - 0s 35us/sample - loss: 0.7538 - loss_classic: 0.7422 - rmse: 0.6348 - rsquare: 0.9954 - val_loss: 6.6834 - val_loss_classic: 6.6709 - val_rmse: 1.9132 - val_rsquare: 0.9591\n",
      "Epoch 80/100\n",
      "6400/6400 [==============================] - 0s 33us/sample - loss: 0.7560 - loss_classic: 0.7443 - rmse: 0.6307 - rsquare: 0.9954 - val_loss: 6.3622 - val_loss_classic: 6.3498 - val_rmse: 1.9048 - val_rsquare: 0.9610\n",
      "Epoch 81/100\n",
      "6400/6400 [==============================] - 0s 38us/sample - loss: 0.5539 - loss_classic: 0.5423 - rmse: 0.5354 - rsquare: 0.9966 - val_loss: 6.7533 - val_loss_classic: 6.7407 - val_rmse: 2.0053 - val_rsquare: 0.9587\n",
      "Epoch 82/100\n",
      "6400/6400 [==============================] - 0s 47us/sample - loss: 0.5806 - loss_classic: 0.5690 - rmse: 0.5378 - rsquare: 0.9965 - val_loss: 6.3054 - val_loss_classic: 6.2927 - val_rmse: 1.9880 - val_rsquare: 0.9613\n",
      "Epoch 83/100\n",
      "6400/6400 [==============================] - 1s 139us/sample - loss: 0.7205 - loss_classic: 0.7090 - rmse: 0.5925 - rsquare: 0.9956 - val_loss: 4.6178 - val_loss_classic: 4.6054 - val_rmse: 1.6114 - val_rsquare: 0.9717\n",
      "Epoch 84/100\n",
      "6400/6400 [==============================] - 0s 27us/sample - loss: 0.7922 - loss_classic: 0.7806 - rmse: 0.6541 - rsquare: 0.9952 - val_loss: 6.2117 - val_loss_classic: 6.1993 - val_rmse: 1.8823 - val_rsquare: 0.9620\n",
      "Epoch 85/100\n",
      "6400/6400 [==============================] - 0s 29us/sample - loss: 0.7899 - loss_classic: 0.7783 - rmse: 0.6338 - rsquare: 0.9951 - val_loss: 7.5374 - val_loss_classic: 7.5250 - val_rmse: 1.9915 - val_rsquare: 0.9539\n",
      "Epoch 86/100\n",
      "6400/6400 [==============================] - 0s 30us/sample - loss: 0.8353 - loss_classic: 0.8237 - rmse: 0.6503 - rsquare: 0.9949 - val_loss: 5.8969 - val_loss_classic: 5.8847 - val_rmse: 1.8010 - val_rsquare: 0.9640\n",
      "Epoch 87/100\n",
      "6400/6400 [==============================] - 0s 24us/sample - loss: 0.6413 - loss_classic: 0.6297 - rmse: 0.5762 - rsquare: 0.9961 - val_loss: 4.6790 - val_loss_classic: 4.6667 - val_rmse: 1.6245 - val_rsquare: 0.9715\n",
      "Epoch 88/100\n",
      "6400/6400 [==============================] - 2s 291us/sample - loss: 0.8337 - loss_classic: 0.8222 - rmse: 0.6413 - rsquare: 0.9949 - val_loss: 4.4331 - val_loss_classic: 4.4208 - val_rmse: 1.5402 - val_rsquare: 0.9729\n",
      "Epoch 89/100\n",
      "6400/6400 [==============================] - 0s 53us/sample - loss: 0.7459 - loss_classic: 0.7343 - rmse: 0.6454 - rsquare: 0.9954 - val_loss: 5.4580 - val_loss_classic: 5.4457 - val_rmse: 1.6125 - val_rsquare: 0.9665\n",
      "Epoch 90/100\n",
      "6400/6400 [==============================] - 0s 43us/sample - loss: 0.7688 - loss_classic: 0.7573 - rmse: 0.6364 - rsquare: 0.9953 - val_loss: 11.4792 - val_loss_classic: 11.4665 - val_rmse: 2.7493 - val_rsquare: 0.9295\n",
      "Epoch 91/100\n",
      "6400/6400 [==============================] - 0s 68us/sample - loss: 0.7309 - loss_classic: 0.7193 - rmse: 0.6385 - rsquare: 0.9955 - val_loss: 7.8483 - val_loss_classic: 7.8358 - val_rmse: 2.0268 - val_rsquare: 0.9520\n",
      "Epoch 92/100\n",
      "6400/6400 [==============================] - 0s 22us/sample - loss: 0.8729 - loss_classic: 0.8613 - rmse: 0.6674 - rsquare: 0.9946 - val_loss: 11.6269 - val_loss_classic: 11.6143 - val_rmse: 2.8035 - val_rsquare: 0.9282\n",
      "Epoch 93/100\n",
      "6400/6400 [==============================] - 0s 34us/sample - loss: 0.8351 - loss_classic: 0.8236 - rmse: 0.6606 - rsquare: 0.9949 - val_loss: 12.7630 - val_loss_classic: 12.7502 - val_rmse: 2.7058 - val_rsquare: 0.9217\n",
      "Epoch 94/100\n",
      "6400/6400 [==============================] - 0s 34us/sample - loss: 0.5525 - loss_classic: 0.5408 - rmse: 0.5358 - rsquare: 0.9966 - val_loss: 12.2119 - val_loss_classic: 12.1992 - val_rmse: 2.7824 - val_rsquare: 0.9252\n",
      "Epoch 95/100\n",
      "6400/6400 [==============================] - 0s 24us/sample - loss: 0.5747 - loss_classic: 0.5631 - rmse: 0.5427 - rsquare: 0.9965 - val_loss: 8.3836 - val_loss_classic: 8.3712 - val_rmse: 2.2372 - val_rsquare: 0.9485\n",
      "Epoch 96/100\n",
      "6400/6400 [==============================] - 0s 37us/sample - loss: 0.7268 - loss_classic: 0.7152 - rmse: 0.6094 - rsquare: 0.9956 - val_loss: 4.3059 - val_loss_classic: 4.2936 - val_rmse: 1.4735 - val_rsquare: 0.9736\n",
      "Epoch 97/100\n",
      "6400/6400 [==============================] - 0s 30us/sample - loss: 0.8231 - loss_classic: 0.8115 - rmse: 0.6525 - rsquare: 0.9949 - val_loss: 4.4431 - val_loss_classic: 4.4309 - val_rmse: 1.5667 - val_rsquare: 0.9728\n",
      "Epoch 98/100\n",
      "6400/6400 [==============================] - 0s 25us/sample - loss: 0.6413 - loss_classic: 0.6297 - rmse: 0.5805 - rsquare: 0.9961 - val_loss: 5.4249 - val_loss_classic: 5.4125 - val_rmse: 1.6934 - val_rsquare: 0.9668\n",
      "Epoch 99/100\n",
      "6400/6400 [==============================] - 0s 21us/sample - loss: 0.6915 - loss_classic: 0.6799 - rmse: 0.5743 - rsquare: 0.9958 - val_loss: 4.7519 - val_loss_classic: 4.7398 - val_rmse: 1.6018 - val_rsquare: 0.9708\n",
      "Epoch 100/100\n",
      "6400/6400 [==============================] - 0s 22us/sample - loss: 0.9333 - loss_classic: 0.9217 - rmse: 0.7172 - rsquare: 0.9943 - val_loss: 6.2742 - val_loss_classic: 6.2623 - val_rmse: 1.8821 - val_rsquare: 0.9616\n",
      "Training done!\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 4.3847 - loss_classic: 4.3817 - rmse: 1.4899 - rsquare: 0.9706\n"
     ]
    }
   ],
   "source": [
    "epoch=100\n",
    "model_layers=(140,60,30,20,)\n",
    "tf.reset_default_graph()\n",
    "#Model\n",
    "gamma=0.000001\n",
    "beta=0.0001\n",
    "x_train, x_val = x[:x.shape[0]*80//100], x[x.shape[0]*80//100:]\n",
    "y_train, y_val = y[:x.shape[0]*80//100], y[x.shape[0]*80//100:]\n",
    "\n",
    "\n",
    "#Create the model\n",
    "inputs=tf.keras.Input(shape=(x_train.shape[1],))\n",
    "network=LINA_Network(num_feature=x_train.shape[1],list_reduc=model_layers,l2_strength=beta,dropout_act=False,classification=False,bn=True)\n",
    "p,logit,A_tensor,dA_dx_tensor,dy_dx_tensor=network(inputs)\n",
    "model=tf.keras.Model(inputs,p)\n",
    "\n",
    "#losses\n",
    "def loss_with_custom_regularization(y_true, y_pred):\n",
    "    cross_entropy=tf.keras.losses.MSE(y_true, y_pred)\n",
    "    a_regularization=tf.math.scalar_mul(gamma,tf.reduce_sum(tf.abs(A_tensor-1)))\n",
    "    loss_loc=cross_entropy+a_regularization\n",
    "    return loss_loc\n",
    "\n",
    "def loss_classic(y_true, y_pred):\n",
    "    loss_loc=tf.keras.losses.MSE(y_true, y_pred)\n",
    "    return loss_loc\n",
    "\n",
    "filepath=\"./model.hdf5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss_classic',verbose=0, save_best_only=True, mode='min',save_weights_only=True)\n",
    "csv=tf.keras.callbacks.CSVLogger('model.log')\n",
    "callbacks_list = [checkpoint,csv]\n",
    "adam = tf.keras.optimizers.Adam(lr = 0.1)\n",
    "model.compile(loss=loss_with_custom_regularization,optimizer=adam,metrics=[loss_classic,rmse,rsquare])\n",
    "\n",
    "#Train Model\n",
    "print(\"Training...\")\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),callbacks=callbacks_list, epochs=epoch, batch_size=512,verbose=True)\n",
    "\n",
    "#get the best model back\n",
    "model.load_weights(filepath)\n",
    "print(\"Training done!\")\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "A_model=tf.keras.Model(inputs,A_tensor)\n",
    "dy_dx_model=tf.keras.Model(inputs,dy_dx_tensor)\n",
    "dA_dx_model=tf.keras.Model(inputs,dA_dx_tensor)\n",
    "\n",
    "#Get the values\n",
    "A_values=A_model.predict(x_test)\n",
    "A_jacobian_values=np.array(dA_dx_model.predict(x_test))\n",
    "dy_dx_values=dy_dx_model.predict(x_test)\n",
    "K_values=network.logit.get_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Get Values of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-20*np.sin(2*X1*X2+X8)+ 2*np.abs(X3)+ 4*X4*X5- np.exp(-X6)'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''-20*np.sin(2*X1*X2+X8)+ 2*np.abs(X3)+ 4*X4*X5- np.exp(-X6)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Get Values of Interest Model Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: {'x1': 15.60287, 'x2': 15.25648, 'x8': 14.563858, 'x5': 2.4373538, 'x4': 2.3859043, 'x3': 2.1120348, 'x6': 1.0976211, 'x9': 0.18200284, 'x7': 0.14876302, 'x10': 0.10126297}\n",
      "\n",
      "\n",
      "Feature Interactions:\n",
      "{'x1,x2': 27.922777, 'x1,x8': 10.089996, 'x2,x8': 9.660543, 'x4,x5': 4.4089317, 'x1,x5': 1.1890814, 'x2,x3': 1.1735195, 'x1,x3': 1.1656184, 'x1,x4': 1.0764585, 'x2,x4': 1.0182382, 'x2,x5': 0.9568363, 'x3,x8': 0.8902999, 'x4,x8': 0.7763662, 'x5,x8': 0.65677476, 'x3,x4': 0.5516539, 'x3,x5': 0.45929167, 'x6,x8': 0.4010238, 'x2,x6': 0.397225, 'x1,x6': 0.3653586, 'x8,x9': 0.23825678, 'x2,x7': 0.20970607, 'x2,x9': 0.18354887, 'x1,x9': 0.17468007, 'x7,x8': 0.12749225, 'x4,x6': 0.124263875, 'x1,x7': 0.11853514, 'x5,x6': 0.111195534, 'x8,x10': 0.09822282, 'x1,x10': 0.086221546, 'x2,x10': 0.079726644, 'x4,x7': 0.06608707, 'x5,x7': 0.0656311, 'x5,x9': 0.057361405, 'x5,x10': 0.05386252, 'x4,x9': 0.047952622, 'x4,x10': 0.036822796, 'x3,x6': 0.034314122, 'x3,x9': 0.027441563, 'x3,x7': 0.02579764, 'x6,x9': 0.0062142154, 'x6,x7': 0.006095993, 'x7,x9': 0.0025952572, 'x3,x10': 0.0017634876, 'x9,x10': 0.0014896372, 'x7,x10': 0.0011042905, 'x6,x10': 0.0010966589}\n"
     ]
    }
   ],
   "source": [
    "#Get Importances\n",
    "columns=[\"x\"+str(count+1) for count in range(x_train.shape[1])]\n",
    "derivA_tot=[]\n",
    "for j in range(x_test.shape[1]):\n",
    "    sum_tot=K_values[0][:,0][j]*A_values[:,j]\n",
    "    for k in range(x_test.shape[1]):\n",
    "        sum_tot+=K_values[0][:,0][k]*A_jacobian_values[k,:,j]*x_test[:,k]\n",
    "    derivA_tot.append(np.mean(np.abs(sum_tot)))\n",
    "\n",
    "    \n",
    "#Print Importances\n",
    "feature_ranked=[]\n",
    "for i in np.argsort(derivA_tot)[::-1]:\n",
    "    feature_ranked.append(columns[i])\n",
    "features_values={}\n",
    "for i in range(len(derivA_tot)):\n",
    "    features_values[columns[i]]=derivA_tot[i]\n",
    "print(\"Feature Importances:\",dict(sorted(features_values.items(), key=lambda item: item[1],reverse=True)))\n",
    "print(\"\\n\")\n",
    "#Get Interactions\n",
    "all_inter=[]\n",
    "data_tot=[]\n",
    "strengths={}\n",
    "for i in range(x_train.shape[1]):\n",
    "    for j in range(i+1,x_train.shape[1]):\n",
    "        strengths[\"x\"+str(i+1)+\",x\"+str(j+1)]=np.mean(np.abs(K_values[0][i,0]*A_jacobian_values[i,:,j]+K_values[0][j,0]*A_jacobian_values[j,:,i]),axis=0)\n",
    "\n",
    "#Sort results and print Interactions     \n",
    "topk_inter=dict(sorted(strengths.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Feature Interactions:\")\n",
    "print(topk_inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Get Values of Interest Instance Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances Instance: {'x8': 21.515720154856222, 'x1': 20.96426279547158, 'x5': 4.4477916008754175, 'x3': 2.915674250028312, 'x4': 1.9255991461788886, 'x6': 1.6438706653015394, 'x2': 1.51338001411221, 'x7': 0.2409585071447216, 'x10': 0.13361849468329573, 'x9': 0.04456344370715947}\n",
      "\n",
      "\n",
      "Feature Interactions Instance:\n",
      "{'x1,x2': 50.39241, 'x4,x5': 5.300459, 'x2,x8': 4.3560123, 'x1,x8': 3.4556975, 'x2,x3': 3.0945563, 'x1,x4': 1.9398233, 'x3,x8': 1.6308753, 'x4,x8': 1.4843621, 'x3,x5': 1.1157187, 'x3,x4': 0.9667753, 'x1,x5': 0.816437, 'x6,x8': 0.7960988, 'x1,x6': 0.68383706, 'x5,x8': 0.51972514, 'x2,x7': 0.50811803, 'x5,x6': 0.46610478, 'x1,x3': 0.448335, 'x8,x9': 0.35809895, 'x2,x4': 0.3545314, 'x2,x9': 0.33640534, 'x2,x5': 0.32252577, 'x4,x6': 0.27049884, 'x1,x10': 0.16085628, 'x1,x9': 0.13356012, 'x4,x7': 0.12827723, 'x1,x7': 0.11663456, 'x2,x10': 0.09953439, 'x3,x6': 0.08500106, 'x5,x7': 0.08280718, 'x4,x10': 0.0692056, 'x7,x8': 0.06158585, 'x8,x10': 0.053611908, 'x5,x10': 0.05094219, 'x2,x6': 0.04437551, 'x3,x7': 0.025901426, 'x4,x9': 0.022255128, 'x3,x9': 0.02024232, 'x6,x7': 0.012954128, 'x5,x9': 0.009264862, 'x6,x9': 0.008357802, 'x9,x10': 0.002548674, 'x3,x10': 0.0024403746, 'x6,x10': 0.0023765184, 'x7,x10': 0.0012443461, 'x7,x9': 0.0011505631}\n"
     ]
    }
   ],
   "source": [
    "#Get Importances\n",
    "instance=50\n",
    "columns=[\"x\"+str(count+1) for count in range(x_train.shape[1])]\n",
    "derivA_tot=[]\n",
    "for j in range(x_test.shape[1]):\n",
    "    sum_tot=K_values[0][:,0][j]*A_values[instance,j]\n",
    "    for k in range(x_test.shape[1]):\n",
    "        sum_tot+=K_values[0][:,0][k]*A_jacobian_values[k,instance,j]*x_test[instance,k]\n",
    "    derivA_tot.append(np.abs(sum_tot))\n",
    "\n",
    "    \n",
    "#Print Importances\n",
    "feature_ranked=[]\n",
    "for i in np.argsort(derivA_tot)[::-1]:\n",
    "    feature_ranked.append(columns[i])\n",
    "features_values={}\n",
    "for i in range(len(derivA_tot)):\n",
    "    features_values[columns[i]]=derivA_tot[i]\n",
    "print(\"Feature Importances Instance:\",dict(sorted(features_values.items(), key=lambda item: item[1],reverse=True)))\n",
    "print(\"\\n\")\n",
    "#Get Interactions\n",
    "all_inter=[]\n",
    "data_tot=[]\n",
    "strengths={}\n",
    "for i in range(x_train.shape[1]):\n",
    "    for j in range(i+1,x_train.shape[1]):\n",
    "        strengths[\"x\"+str(i+1)+\",x\"+str(j+1)]=np.abs(K_values[0][i,0]*A_jacobian_values[i,instance,j]+K_values[0][j,0]*A_jacobian_values[j,instance,i])\n",
    "#Sort results and print Interactions     \n",
    "topk_inter=dict(sorted(strengths.items(), key=lambda item: item[1],reverse=True))\n",
    "print(\"Feature Interactions Instance:\")\n",
    "print(topk_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
