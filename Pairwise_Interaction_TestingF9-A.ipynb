{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import sys  \n",
    "sys.path.insert(0, './')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#Local Libraries\n",
    "from data_manager import DatasetManager\n",
    "from attention_machine import AttentionNetwork\n",
    "from feed_forward import FeedForward\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.python.ops.parallel_for.gradients import batch_jacobian\n",
    "try:\n",
    "        tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=DatasetManager(5000,13,version=\"F4\",factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y,data=dm.generate_dataset(display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data[0:data.shape[0]*80//100]\n",
    "x_test=data[data.shape[0]*80//100:]\n",
    "y=Y[0:data.shape[0]*80//100,:]\n",
    "y_test=Y[data.shape[0]*80//100:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anyorder_R_precision(interactions, ground_truth):\n",
    "\n",
    "    R = len(ground_truth)\n",
    "    recovered_gt = []\n",
    "    counter = 0\n",
    "\n",
    "    for inter, strength in interactions:\n",
    "\n",
    "        inter_set = set(inter)  # assume 1-indexed\n",
    "        #print(inter_set)\n",
    "        #print(ground_truth)\n",
    "        if any(inter_set <= gt for gt in ground_truth):\n",
    "            #print(\"Good\")\n",
    "            \n",
    "            counter += 1\n",
    "    R_precision = counter / R\n",
    "\n",
    "    return R_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_auc(interactions, ground_truth):\n",
    "    strengths = []\n",
    "    gt_binary_list = []\n",
    "    for inter, strength in interactions:\n",
    "        inter_set = set(inter)  # assume 1-indexed\n",
    "        strengths.append(strength)\n",
    "        #print(inter_set)\n",
    "        #print(ground_truth)\n",
    "        if any(inter_set <= gt for gt in ground_truth):\n",
    "            #print(\"Good\")\n",
    "            gt_binary_list.append(1)\n",
    "        else:\n",
    "            #print(\"Bad\")\n",
    "            gt_binary_list.append(0)\n",
    "    print(gt_binary_list)\n",
    "    auc = roc_auc_score(gt_binary_list, strengths)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=0\n",
    "for train_index, val_index in kf.split(x):\n",
    "    x_train, x_val = x[train_index], x[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    np.savetxt('y_trainF4'+str(run)+'.csv',y_train)\n",
    "    np.savetxt('x_trainF4'+str(run)+'.csv',x_train)\n",
    "    np.savetxt('y_valF4'+str(run)+'.csv',y_val)\n",
    "    np.savetxt('x_valF4'+str(run)+'.csv',x_val)\n",
    "    run+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('y_testF4.csv',y_test)\n",
    "np.savetxt('x_testF4.csv',x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=0\n",
    "epoch=150\n",
    "top_num=4\n",
    "\n",
    "model_layers=[140, 100, 60,20]\n",
    "results=[]\n",
    "gt=[{1,2},{3,4},{1,3},{1,4},{2,3},{2,4},{1,5},{2,5},{3,5},{4,5},{6,7},{6,8},{7,8},{9,10}]\n",
    "top=len(gt)\n",
    "rankings=[]\n",
    "x_test=np.genfromtxt(\"x_testF4.csv\")\n",
    "y_test=np.genfromtxt(\"y_testF4.csv\")\n",
    "for i in range(10):\n",
    "    print(\"Model:\"+str(run))\n",
    "    x_train=np.genfromtxt(\"x_trainF4\"+str(i)+\".csv\")\n",
    "    y_train=np.genfromtxt(\"y_trainF4\"+str(i)+\".csv\")\n",
    "    x_val=np.genfromtxt(\"x_valF4\"+str(i)+\".csv\")\n",
    "    y_val=np.genfromtxt(\"y_valF4\"+str(i)+\".csv\")\n",
    "    tf.reset_default_graph()\n",
    "    #Model\n",
    "    gamma=0.0000001\n",
    "    beta=0.0001\n",
    "    inputs=tf.keras.Input(shape=(x_train.shape[1],))\n",
    "    network=AttentionNetwork(num_feature=x_train.shape[1],list_reduc=model_layers,l2_strength=beta,dropout_act=False,classification=False)\n",
    "    p,logit,AX,A,dA_dx,dp_dx,dH_dx=network(inputs)\n",
    "    model=tf.keras.Model(inputs,p)\n",
    "    #losses\n",
    "    def loss_with_custom_regularization(y_true, y_pred):\n",
    "        cross_entropy=tf.keras.losses.MSE(y_true, y_pred)\n",
    "        a_regularization=tf.math.scalar_mul(gamma,tf.reduce_sum(tf.abs(A-1)))\n",
    "        loss_loc=cross_entropy+a_regularization\n",
    "        return loss_loc\n",
    "\n",
    "    def loss_classic(y_true, y_pred):\n",
    "        loss_loc=tf.keras.losses.MSE(y_true, y_pred)\n",
    "        return loss_loc\n",
    "\n",
    "    filepath=\"./attention_machine_non_linearF4\"+str(run)+\"A.hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',verbose=0, save_best_only=True, mode='min',save_weights_only=True)\n",
    "    csv=tf.keras.callbacks.CSVLogger('./attention_machine_non_linearF4'+str(run)+'A.log')\n",
    "    run+=1\n",
    "\n",
    "    callbacks_list = [checkpoint,csv]\n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.01)\n",
    "    model.compile(loss=loss_with_custom_regularization,optimizer=adam,metrics=[loss_classic])\n",
    "    \n",
    "    #Train Model\n",
    "    print(\"Training...\")\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val),callbacks=callbacks_list, epochs=epoch, batch_size=32,verbose=True)\n",
    "    \n",
    "    #get the best model back\n",
    "    model.load_weights(filepath)\n",
    "    print(\"Training done!\")\n",
    "    model.evaluate(x_test, y_test)\n",
    "\n",
    "    jac_tensor=batch_jacobian(A,inputs,use_pfor=False)\n",
    "    w=network.logit.get_weights()\n",
    "    sess=tf.compat.v1.keras.backend.get_session()\n",
    "    grad_val=jac_tensor.eval(feed_dict={inputs:x_test},session=sess)\n",
    "    #get interactions\n",
    "    all_inter=[]\n",
    "    data_tot=[]\n",
    "    strengths={}\n",
    "    for i in range(x_train.shape[1]):\n",
    "        for j in range(i+1,x_train.shape[1]):\n",
    "            strengths[str(i+1)+\",\"+str(j+1)]=np.mean(np.abs(w[0][j,0]*grad_val[:,j,i]+w[0][i,0]*grad_val[:,i,j]),axis=0)\n",
    "    topk_inter=dict(sorted(strengths.items(), key=lambda item: item[1],reverse=True)[:top])\n",
    "    print(topk_inter)\n",
    "    dictlist=[]\n",
    "    rankings.append(dict(sorted(strengths.items(), key=lambda item: item[1],reverse=True)))\n",
    "    for key, value in topk_inter.items():\n",
    "        temp = ((int(key.split(\",\")[0]),int(key.split(\",\")[1])),value)\n",
    "        dictlist.append(temp)\n",
    "    R=get_anyorder_R_precision(dictlist,gt)\n",
    "    print(\"R:\",R)\n",
    "    results.append(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=np.mean(results)\n",
    "sd=np.std(results)\n",
    "print(mean,sd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
