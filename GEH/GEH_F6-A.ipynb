{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_array(array):\n",
    "    temp = array.argsort()\n",
    "    ranks = np.empty_like(temp)\n",
    "    ranks[temp] = np.arange(len(array))\n",
    "    return ranks\n",
    "def matrix2vec(matrix):\n",
    "    d = len(matrix)\n",
    "    interactions = np.zeros([int(d * (d-1) / 2),1])\n",
    "    k = 0\n",
    "    for i in range(d):\n",
    "        for j in range(i+1, d):\n",
    "            interactions[k] = matrix[i][j]\n",
    "            k = k + 1  \n",
    "    return interactions\n",
    "def matric2dic(hessian, K):\n",
    "    IS = {}\n",
    "    for i in range(len(hessian[0])):\n",
    "        for j in range(i+1, len(hessian[0])):\n",
    "            tmp = 0\n",
    "            interation = 'Interaction: '\n",
    "            interation = interation + str(i + 1) + ' ' + str(j + 1) + ' '\n",
    "            IS[interation] = hessian[i][j]\n",
    "    Sorted_IS = [(k, IS[k]) for k in sorted(IS, key=IS.get, reverse=True)]\n",
    "    return IS, Sorted_IS\n",
    "def vec2dic(vector, num_dim,len_gt):\n",
    "    IS = {}\n",
    "    tmp = 0\n",
    "    for i in range(num_dim):\n",
    "        for j in range(i+1, num_dim):\n",
    "            interation = str(i + 1) + ',' + str(j + 1) \n",
    "            IS[interation] = vector[tmp]\n",
    "            tmp = tmp + 1\n",
    "    Sorted_IS = dict(sorted(IS.items(), key=lambda item: item[1],reverse=True)[:len_gt])\n",
    "\n",
    "    dictlist=[]\n",
    "    for key, value in Sorted_IS.items():\n",
    "        temp = ((int(key.split(\",\")[0]),int(key.split(\",\")[1])),value)\n",
    "        dictlist.append(temp)\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data (Energy efficiency Data Set from UCI)\n",
    "Data is from: A. Tsanas, A. Xifara: 'Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools', Energy and Buildings, Vol. 49, pp. 560-567, 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data preprocessing\n",
    "def data_generating_energy(traindf, testdf):\n",
    "\n",
    "    trainx = traindf.drop('Y2', axis=1).values; trainy = traindf['Y2'].values\n",
    "    testx = testdf.drop('Y2', axis=1).values; testy = testdf['Y2'].values\n",
    "\n",
    "    scalerx = preprocessing.MinMaxScaler().fit(trainx)\n",
    "    trainx = scalerx.transform(trainx)+ 0.5; testx = scalerx.transform(testx)+ 0.5\n",
    "\n",
    "    scalery = preprocessing.MinMaxScaler().fit(trainy.reshape(-1,1))\n",
    "    trainy = scalery.transform(trainy.reshape(-1,1)).reshape(1, -1); testy = scalery.transform(testy.reshape(-1,1)).reshape(1, -1)\n",
    "\n",
    "    # transfer data into tensor\n",
    "    print(trainy,trainx)\n",
    "    x = torch.tensor(trainx, dtype = torch.float); y = torch.tensor(trainy[0], dtype = torch.float)\n",
    "    x_test = torch.tensor(testx, dtype = torch.float); y_test = torch.tensor(testy[0], dtype = torch.float)\n",
    "    \n",
    "    return x, y, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interaction effect measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Group Expected Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEH\n",
    "def inputHessian_dropout(mlp, r):\n",
    "    Hessian = []\n",
    "    output = mlp(r)\n",
    "    first = torch.autograd.grad(output, r, create_graph=True)\n",
    "    for i in range(len(r[0])):\n",
    "        gradient = torch.zeros(len(r[0]), dtype = torch.float)\n",
    "        gradient[i] = 1.0\n",
    "        second = torch.autograd.grad(first, r, grad_outputs=gradient.view(1,-1), retain_graph=True)\n",
    "        Hessian.append(second[0][0].tolist())\n",
    "    return Hessian\n",
    "def AEH(mlp, num_dim, data):\n",
    "    Hessian = np.zeros([num_dim, num_dim])\n",
    "    n_data = data.shape[0]\n",
    "    for i in range(n_data):\n",
    "        r = data[i].clone().detach().requires_grad_(True).view(1,-1)\n",
    "        Hessian = Hessian + np.array(inputHessian_dropout(mlp, r))\n",
    "    Hessian = abs(Hessian) / n_data \n",
    "    HS, sorted_HS = matric2dic(Hessian, 10)\n",
    "    return sorted_HS, Hessian\n",
    "\n",
    "## Group Expected Hessian\n",
    "def GEH(mlp, data, num_cluster,feature_size):\n",
    "    kmeans = KMeans(n_clusters=num_cluster, random_state=0).fit(data)\n",
    "    data_sep = []\n",
    "    data_number = 0\n",
    "    for i in range(num_cluster):\n",
    "        data_i = data[(kmeans.labels_ == i).tolist()]; data_i = torch.tensor(data_i, dtype=torch.float)\n",
    "        data_sep.append(data_i)\n",
    "        data_number = data_number + data_i.shape[0]\n",
    "    mlp.eval()\n",
    "    Hessian = np.zeros([num_feature, num_feature])\n",
    "    for i in range(num_cluster):\n",
    "        _, hessian = AEH(mlp, num_feature, data = data_sep[i])\n",
    "        Hessian = Hessian + hessian * data_sep[i].shape[0] / data_number  \n",
    "    return Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bayesian Group Expected Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian GEH\n",
    "def MC_inputHessian(mlp, r, fixed_noise):\n",
    "    Hessian = []\n",
    "    output = mlp(r, fixed_noise)\n",
    "    first = torch.autograd.grad(output, r, create_graph=True)\n",
    "    for i in range(len(r[0])):\n",
    "        gradient = torch.zeros(len(r[0]), dtype = torch.float)\n",
    "        gradient[i] = 1.0\n",
    "        second = torch.autograd.grad(first, r, grad_outputs=gradient.view(1,-1), retain_graph=True)\n",
    "        Hessian.append(second[0][0].tolist())\n",
    "    return Hessian\n",
    "\n",
    "def MC_AEH(mlp, num_dim, fixed_noise, data):\n",
    "    Hessian = np.zeros([num_dim, num_dim])\n",
    "    n_data = data.shape[0]\n",
    "    for i in range(n_data):    \n",
    "        r = data[i].clone().detach().requires_grad_(True).view(1,-1)\n",
    "        Hessian = Hessian + np.array(MC_inputHessian(mlp, r, fixed_noise))\n",
    "    Hessian = abs(Hessian) / n_data\n",
    "    HS, sorted_HS = matric2dic(Hessian, num_dim)\n",
    "    return sorted_HS, Hessian\n",
    "\n",
    "def MC_GEH_cluster(mlp, data_sep, num_cluster,feature_size):\n",
    "           \n",
    "    mlp.train()\n",
    "    fixed_noise = [torch.rand(feature_size),torch.rand(140),torch.rand(100),torch.rand(60),\n",
    "                   torch.rand(20), torch.rand(10)]\n",
    "    Hessian = np.zeros([num_feature, num_feature])\n",
    "    data_number = 0\n",
    "    for i in range(num_cluster):\n",
    "        data_number = data_number + data_sep[i].shape[0]\n",
    "        \n",
    "    for i in range(num_cluster):\n",
    "        _, hessian = MC_AEH(mlp, num_feature, fixed_noise, data = data_sep[i])\n",
    "        Hessian = Hessian + hessian * data_sep[i].shape[0] / data_number\n",
    "        \n",
    "    return Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Determine the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_rank(Hessian0, Hessian1):\n",
    "    length = Hessian0.shape[0]\n",
    "    lti = np.tril_indices(length, -1)\n",
    "    attribution0 = Hessian0[lti] / np.sum(Hessian0[lti]); attribution1 = Hessian1[lti] / np.sum(Hessian1[lti])\n",
    "    rank0 = sort_array(attribution0); rank1 = sort_array(attribution1)\n",
    "    return np.sum(((attribution0-attribution1) ** 2 ) * ((rank0 - rank1) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_M(mlp, data, NUM_Cluster = 60):\n",
    "    increase = []\n",
    "    Hessian0 = GEH(mlp, data, 1)\n",
    "    for i in range(NUM_Cluster - 1):\n",
    "        num_cluster = i + 2\n",
    "        hessian = GEH(mlp, data, num_cluster)\n",
    "        increase.append(spearman_rank(Hessian0,hessian))\n",
    "        Hessian0 = hessian\n",
    "    return np.array(increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Interaction via Concrete Dropout\n",
    "Gal, Yarin, Jiri Hron, and Alex Kendall. \"Concrete dropout.\" Advances in neural information processing systems. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### We add a main effect to improve the NN training, which could be omited as well.\n",
    "class Main_effect(nn.Module):\n",
    "    def __init__(self, num_dim):\n",
    "        super(Main_effect, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_dim, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from the original concrete dropout, we control the random mask of each node manually by providing the noise. This is used in Bayesian GEH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete Dropout Layer\n",
    "class ConcreteDropout(nn.Module):\n",
    "    def __init__(self, size = 1, weight_regularizer=1e-6,\n",
    "                 dropout_regularizer=1e-5, init_min=0.1, init_max=0.1):\n",
    "        \n",
    "        super(ConcreteDropout, self).__init__()\n",
    "        \n",
    "        self.weight_regularizer = weight_regularizer\n",
    "        self.dropout_regularizer = dropout_regularizer\n",
    "        \n",
    "        init_min = np.log(init_min) - np.log(1. - init_min)\n",
    "        init_max = np.log(init_max) - np.log(1. - init_max)\n",
    "        \n",
    "        self.p_logit = nn.Parameter(torch.empty(size).uniform_(init_min, init_max))\n",
    "\n",
    "    def forward(self, x, layer, noise):\n",
    "        p = torch.sigmoid(self.p_logit)\n",
    "            \n",
    "        out = layer(self._concrete_dropout(x, p, noise))\n",
    "\n",
    "        sum_of_square = 0\n",
    "        \n",
    "        network_weights = torch.sum(torch.sum(torch.pow(layer.weight, 2), 0) / (1 - p))\n",
    "        network_bias = torch.sum(torch.pow(layer.bias, 2))   \n",
    "            \n",
    "        weights_regularizer = self.weight_regularizer * (network_bias + network_weights)\n",
    "\n",
    "        dropout_regularizer = p * torch.log(p) + (1. - p) * torch.log(1. - p)\n",
    "        dropout_regularizer = self.dropout_regularizer * torch.sum(dropout_regularizer)\n",
    "\n",
    "        regularization = weights_regularizer + dropout_regularizer\n",
    "\n",
    "        return out, regularization\n",
    "        \n",
    "    def _concrete_dropout(self, x, p, noise):\n",
    "        eps = 1e-7\n",
    "        temp = 0.1\n",
    "        \n",
    "        if type(noise) == int: ## shape of unif_noise is [num_data, dim]\n",
    "            unif_noise = torch.rand_like(x)\n",
    "        else:\n",
    "            unif_noise = noise\n",
    "            \n",
    "        drop_prob = (torch.log(p + eps)\n",
    "                    - torch.log(1 - p + eps)\n",
    "                    + torch.log(unif_noise + eps)\n",
    "                    - torch.log(1 - unif_noise + eps))\n",
    "        \n",
    "        drop_prob = torch.sigmoid(drop_prob / temp)\n",
    "        random_tensor = 1 - drop_prob\n",
    "        retain_prob = 1 - p\n",
    "        \n",
    "        x  = torch.mul(x, random_tensor)\n",
    "        x /= retain_prob\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP with concrete dropout layer\n",
    "class Model_NC_softplus(nn.Module):\n",
    "    def __init__(self, weight_regularizer, dropout_regularizer,input_dimension):\n",
    "        super(Model_NC_softplus, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dimension, 140)\n",
    "        self.linear2 = nn.Linear(140, 100)\n",
    "        self.linear3 = nn.Linear(100, 60)\n",
    "        self.linear4 = nn.Linear(60, 20)\n",
    "        self.linear5 = nn.Linear(20, 10)\n",
    "        self.linear6 = nn.Linear(10, 1)\n",
    "\n",
    "        self.conc_drop1 = ConcreteDropout(size = input_dimension,  weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop2 = ConcreteDropout(size = 140, weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop3 = ConcreteDropout(size = 100, weight_regularizer=weight_regularizer,\n",
    "                                             dropout_regularizer=dropout_regularizer)    \n",
    "        self.conc_drop4 = ConcreteDropout(size = 60,  weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop5 = ConcreteDropout(size = 20, weight_regularizer=weight_regularizer,\n",
    "                                          dropout_regularizer=dropout_regularizer)\n",
    "        self.conc_drop6 = ConcreteDropout(size = 10, weight_regularizer=weight_regularizer,\n",
    "                                             dropout_regularizer=dropout_regularizer)    \n",
    "    def forward(self, x, NOISE = [0,0,0,0,0,0]): ## if the noise is not given, then generate a standard Gaussian noise\n",
    "        softplus = nn.Softplus(beta = 10)\n",
    "        if self.training:\n",
    "            regularization = torch.empty(6)\n",
    "\n",
    "            x1, regularization[0] = self.conc_drop1(x, self.linear1, NOISE[0])\n",
    "            x1 = softplus(x1)\n",
    "            x2, regularization[1] = self.conc_drop2(x1, self.linear2, NOISE[1])\n",
    "            x2 = softplus(x2)\n",
    "            x3, regularization[2] = self.conc_drop3(x2, self.linear3, NOISE[2])\n",
    "            x3 = softplus(x3)\n",
    "            x4, regularization[3] = self.conc_drop4(x3, self.linear4, NOISE[3])\n",
    "            x4 = softplus(x4)\n",
    "            x5, regularization[4] = self.conc_drop5(x4, self.linear5, NOISE[4])\n",
    "            x5 = softplus(x5)\n",
    "            output, regularization[5] = self.conc_drop6(x5, self.linear6, NOISE[5])\n",
    "            return output, regularization.sum()\n",
    "        else:\n",
    "            x = softplus(self.linear1(x))\n",
    "            x = softplus(self.linear2(x))\n",
    "            x = softplus(self.linear3(x))\n",
    "            x = softplus(self.linear4(x))\n",
    "            x = softplus(self.linear5(x))\n",
    "            x = self.linear6(x)            \n",
    "            return x\n",
    "def heteroscedastic_loss(true, output):\n",
    "    return torch.mean(torch.sum((true - output)**2, 1), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Train a Concrete dropout NN\n",
    "About the trainining a Concrete dropout NN, please refer to [Gal et al., 2017] https://github.com/yaringal/ConcreteDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anyorder_R_precision(interactions, ground_truth):\n",
    "\n",
    "    R = len(ground_truth)\n",
    "    recovered_gt = []\n",
    "    counter = 0\n",
    "\n",
    "    for inter, strength in interactions:\n",
    "\n",
    "        inter_set = set(inter)  # assume 1-indexed\n",
    "        #print(inter_set)\n",
    "        #print(ground_truth)\n",
    "        if any(inter_set <= gt for gt in ground_truth):\n",
    "            #print(\"Good\")\n",
    "            \n",
    "            counter += 1\n",
    "    R_precision = counter / R\n",
    "\n",
    "    return R_precision\n",
    "\n",
    "def imuncertainty(mlp, data, num_cluster, n_iter = 20):\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_cluster, random_state=0).fit(data)\n",
    "    data_sep = []\n",
    "    for i in range(num_cluster):\n",
    "        data_i = data[(kmeans.labels_ == i).tolist()]; data_i = torch.tensor(data_i, dtype=torch.float)\n",
    "        data_sep.append(data_i) \n",
    "\n",
    "    print('############# The 1st interation for uncertainty calculation')\n",
    "    # Initialize the first interaction\n",
    "    Hessian = MC_GEH_cluster(mlp, data_sep, num_cluster,data.shape[1])\n",
    "    int_unc = matrix2vec(Hessian)\n",
    "\n",
    "    # Rest of Interaction\n",
    "    for num_int in range(n_iter - 1):\n",
    "        if ((num_int + 2) % 10 == 0):\n",
    "            print('############# The %dth interation for uncertainty calculation' %(num_int + 2))\n",
    "        Hessian = MC_GEH_cluster(mlp, data_sep, num_cluster,data.shape[1])\n",
    "        int_unc_new = matrix2vec(Hessian)\n",
    "    \n",
    "        int_unc = np.concatenate((int_unc, int_unc_new), axis=1)\n",
    "\n",
    "    mean_interaction = np.mean(int_unc, axis = 1)\n",
    "    std_interaction = np.std(int_unc, axis = 1)\n",
    "    #print('mean of each interaction is')\n",
    "    #print(mean_interaction)\n",
    "    #print('std of each interaction is')\n",
    "    #print(std_interaction)\n",
    "    \n",
    "    return mean_interaction, std_interaction\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, tolerance, patience):\n",
    "        self.tolerance = tolerance\n",
    "        self.patience = patience\n",
    "    def stop_criterion(self, val_errors):\n",
    "        if len(val_errors) < self.patience + 1:\n",
    "            return False\n",
    "        else:\n",
    "            current_best = min(val_errors[:-self.patience])\n",
    "            current_stop = True\n",
    "            for i in range(self.patience):\n",
    "                current_stop = current_stop and (val_errors[-i-1] - current_best > self.tolerance)\n",
    "            return current_stop\n",
    "def training_CD_soft(mlp, main_effect, x, y,x_val,y_val, x_test, y_test,gt, learning_rate = 0.001, anneling = 1000, batch_size = 50, num_epoch=150, tolerance=0.002, patience = 20):\n",
    "    parameters = set(main_effect.parameters()) | set(mlp.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr = learning_rate)\n",
    "    early_stop = EarlyStopping(tolerance, patience)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    num_data, num_dim = x.shape\n",
    "    y = y.view(-1, 1)\n",
    "    data = torch.cat((x, y), 1)\n",
    "    results=[]\n",
    "    annel_index = 0\n",
    "    best_loss=999999\n",
    "    for epoch in range(num_epoch):\n",
    "        # permuate the data\n",
    "        data_perm = data[torch.randperm(len(data))]\n",
    "        x = data_perm[:, 0:-1]\n",
    "        y = data_perm[:, -1]\n",
    "\n",
    "        for index in range(int(num_data/batch_size)):\n",
    "            # data comes in\n",
    "            inputs = x[index*batch_size : (index+1)*batch_size]\n",
    "            labels = y[index*batch_size : (index+1)*batch_size].view(-1,1)\n",
    "\n",
    "            # initialize the gradient of optimizer\n",
    "            optimizer.zero_grad()\n",
    "            mlp.train()\n",
    "            # calculate the loss function\n",
    "            output_mlp, reg = mlp(inputs)\n",
    "           # loss with var   \n",
    "#             output_mlp, var, reg = mlp(inputs)          \n",
    "#             loss = heteroscedastic_loss(labels, output_mlp + main_effect(inputs), var) + reg\n",
    "\n",
    "            # calculate the loss function\n",
    "            coef_annel = min(1, 0.01 + annel_index / anneling)\n",
    "        \n",
    "            loss = heteroscedastic_loss(labels, output_mlp + main_effect(inputs)) + coef_annel * reg\n",
    "            # backpropogate the gradient     \n",
    "            loss.backward()\n",
    "            # optimize with SGD\n",
    "            optimizer.step()\n",
    "            \n",
    "            annel_index += 1\n",
    "        # train and validation loss\n",
    "       \n",
    "\n",
    "\n",
    "        if (epoch % 10) == 0:\n",
    "            mlp.eval()\n",
    "            train_errors.append(criterion(mlp.forward(x) + main_effect.forward(x), y.view(-1,1)))\n",
    "            val_errors.append(criterion(mlp.forward(x_val) + main_effect.forward(x_val), y_val.view(-1,1)))\n",
    "            print('EPOCH %d: TRAIN LOSS: %.4f; VAL LOSS IS: %.5f.'% (epoch+1, train_errors[-1], val_errors[-1]))\n",
    "            if val_errors[-1] < best_loss:\n",
    "                best_loss = val_errors[-1]\n",
    "                best_net = copy.deepcopy(mlp)\n",
    "    print(\"Test:\",criterion(best_net.forward(x_test) + main_effect.forward(x_test), y_test.view(-1,1)))\n",
    "    mean_interaction, std_interaction = imuncertainty(best_net, x_test.numpy(),  x_train.shape[1])\n",
    "    Sorted_IS = vec2dic(mean_interaction, x_train.shape[1],len(gt))\n",
    "    R=get_anyorder_R_precision(Sorted_IS,gt)\n",
    "    print(R,Sorted_IS)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.genfromtxt(\"../x_testF1.csv\")\n",
    "y_test=np.genfromtxt(\"../y_testF1.csv\")\n",
    "x_test = torch.tensor(x_test, dtype = torch.float)\n",
    "y_test = torch.tensor(y_test, dtype = torch.float)\n",
    "num_data=x_train.shape[0]\n",
    "num_feature=x_train.shape[1]\n",
    "gt=[{1, 2},{1,3},{2,3}, {3, 5}, {9, 10}, {8, 9},{8,10},{7,9},{7,10},{7, 8},{2,7}]\n",
    "results=[]\n",
    "wr = l**2. /num_data\n",
    "# training parameters:\n",
    "learning_rate = 0.01;\n",
    "l = 1e-7\n",
    "dr = 0.01 * 2 / num_data \n",
    "batch_size = 32; num_epoch = 150;\n",
    "tolerance = 0.01; patience = 20;\n",
    "anneling = 10 * num_data / batch_size\n",
    "for i in range(10):\n",
    "    x_train=np.genfromtxt(\"../x_trainF1\"+str(i)+\".csv\")\n",
    "    y_train=np.genfromtxt(\"../y_trainF1\"+str(i)+\".csv\")\n",
    "    x_val=np.genfromtxt(\"../x_valF1\"+str(i)+\".csv\")\n",
    "    y_val=np.genfromtxt(\"../y_valF1\"+str(i)+\".csv\")\n",
    "    x_train = torch.tensor(x_train, dtype = torch.float) \n",
    "    y_train = torch.tensor(y_train, dtype = torch.float)\n",
    "    x_val = torch.tensor(x_val, dtype = torch.float) \n",
    "    y_val = torch.tensor(y_val, dtype = torch.float)\n",
    "    mlp = Model_NC_softplus(wr, dr,x_train.shape[1])\n",
    "    main_effect = Main_effect(num_feature)\n",
    "    R=training_CD_soft(mlp, main_effect, x_train, y_train,x_val,y_val, x_test, y_test,gt, learning_rate, anneling, batch_size, 150, tolerance, patience)\n",
    "    results.append(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=np.mean(results)\n",
    "sd=np.std(results)\n",
    "print(mean,sd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
