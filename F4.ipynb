{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import sys  \n",
    "sys.path.insert(0, './')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "from sklearn.model_selection import KFold\n",
    "#Local Libraries\n",
    "from data_manager import DatasetManager\n",
    "from attention_machine1 import AttentionNetwork\n",
    "from feed_forward import FeedForward\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "from explain3 import call_L2X\n",
    "from scipy.stats import weightedtau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm=DatasetManager(20000,10,version=\"additive_non_linear_with_product2\",factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y,data=dm.generate_dataset(display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data[0:data.shape[0]*80//100]\n",
    "x_test=data[data.shape[0]*80//100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=Y[0:data.shape[0]*80//100,:]\n",
    "y_test=Y[data.shape[0]*80//100:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10)\n",
    "kf.get_n_splits(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(x):\n",
    "    if len(x.shape)>1:\n",
    "        pred=model.predict(x)\n",
    "    else:\n",
    "        pred=model.predict(np.array([x]))\n",
    "    return np.array([1-pred[:,0],pred[:,0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k(array_deriv,k):\n",
    "    topk=np.argsort(np.mean(array_deriv,axis=0))[::-1][:k]\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(array,k,name,true=np.array([0,0,0,0,1,1,1,2,2,2])):\n",
    "    print(name+\":\")\n",
    "    mean=np.mean(array)\n",
    "    median=np.median(array)\n",
    "    std=np.std(array)\n",
    "    print(\"Mean:\"+str(mean))\n",
    "    print(\"Median:\"+str(median))\n",
    "    print(\"Standard Dev:\"+str(std))\n",
    "    print(\"Standard Error:\"+str(std/np.sqrt(len(array))))\n",
    "    print(\"\\n\")\n",
    "    return name,mean,median,std,std/np.sqrt(len(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kendall(array,true):\n",
    "    rank=np.argsort(array)[::-1]\n",
    "    j=0\n",
    "    print(rank)\n",
    "    for i in rank:\n",
    "        if j<=2:\n",
    "            array[i]=2\n",
    "        elif j>2 and j<6:\n",
    "            array[i]=1\n",
    "        else:\n",
    "            array[i]=0\n",
    "        j+=1\n",
    "    result=weightedtau(array,true,weigher=lambda x: 1 if x<=6 else 0)[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_d(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "ranks_grad=[]\n",
    "ranks_kax=[]\n",
    "ranks_ka=[]\n",
    "ranks_a_ka=[]\n",
    "ranks_a_ka_tot=[]\n",
    "ranks_ax=[]\n",
    "ranks_a=[]\n",
    "ranks_dl=[]\n",
    "ranks_sa=[]\n",
    "ranks_gi=[]\n",
    "ranks_da=[]\n",
    "ranks_LIME=[]\n",
    "epoch=100\n",
    "true=np.array([0,0,0,0,1,1,1,2,2,2])[::-1]\n",
    "top_num=4\n",
    "model_layers=(9,5,)\n",
    "indexes=[]\n",
    "for train_index, val_index in kf.split(x):\n",
    "    print(\"Model:\"+str(i))\n",
    "    indexes.append([train_index,val_index])\n",
    "    x_train, x_val = x[train_index], x[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    tf.reset_default_graph()\n",
    "    #Model\n",
    "    gamma=0.0000001\n",
    "    beta=0.00001\n",
    "    inputs=tf.keras.Input(shape=(x_train.shape[1],))\n",
    "    network=AttentionNetwork(num_feature=x_train.shape[1],list_reduc=model_layers,l2_strength=beta,dropout_act=False,classification=True)\n",
    "    p,logit,AX,A,dA_dx,dp_dx,dH_dx=network(inputs)\n",
    "    model=tf.keras.Model(inputs,p)\n",
    "    #losses\n",
    "    def loss_with_custom_regularization(y_true, y_pred):\n",
    "        cross_entropy=tf.keras.losses.MSE(y_true, y_pred)\n",
    "        a_regularization=tf.math.scalar_mul(gamma,tf.reduce_sum(tf.abs(A-1)))\n",
    "        loss_loc=cross_entropy+a_regularization\n",
    "        return loss_loc\n",
    "\n",
    "    def loss_classication_with_custom_regularization(y_true, y_pred):\n",
    "        cross_entropy=tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "        a_regularization=tf.math.scalar_mul(gamma,tf.reduce_sum(tf.abs(A-1)))\n",
    "        loss_loc=cross_entropy+a_regularization\n",
    "        return loss_loc\n",
    "\n",
    "    def loss_classic(y_true, y_pred):\n",
    "        loss_loc=tf.keras.losses.MSE(y_true, y_pred)\n",
    "        return loss_loc\n",
    "\n",
    "    def loss_classic_classification(y_true, y_pred):\n",
    "        loss_loc=tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "        return loss_loc\n",
    "    filepath=\"./attention_machine_non_linear_with_product2\"+str(i)+\".hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',verbose=0, save_best_only=True, mode='min',save_weights_only=True)\n",
    "    csv=tf.keras.callbacks.CSVLogger('./attention_machine_non_linear_with_product2'+str(i)+'.log')\n",
    "    i+=1\n",
    "    callbacks_list = [checkpoint,csv]\n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.01)\n",
    "    model.compile(loss=loss_classication_with_custom_regularization,optimizer=adam,metrics=[loss_classic_classification])\n",
    "    \n",
    "    #Train Model\n",
    "    print(\"Training...\")\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val),callbacks=callbacks_list, epochs=epoch, batch_size=512,verbose=True)\n",
    "    \n",
    "    #get the best model back\n",
    "    model.load_weights(filepath)\n",
    "    print(\"Training done!\")\n",
    "    model.evaluate(x_test, y_test)\n",
    "    \n",
    "    #output the desired values\n",
    "    model2=tf.keras.Model(inputs,A)\n",
    "    model3=tf.keras.Model(inputs,AX)\n",
    "    model5=tf.keras.Model(inputs,dp_dx)\n",
    "    a=model2.predict(x_test)\n",
    "    h=model3.predict(x_test)\n",
    "    derivative=model5.predict(x_test)\n",
    "    w=network.logit.get_weights()\n",
    "    grad=tf.keras.Model(inputs,dA_dx)\n",
    "    grad_val=np.array(grad.predict(x_test))\n",
    "    #Get dA_dX jacobian\n",
    "    derivA=[]\n",
    "    derivA_tot=[]\n",
    "    derivA_alone=[]\n",
    "    \n",
    "    for j in range(x_test.shape[1]):\n",
    "        sum_tot=w[0][:,0][j]*a[:,j]\n",
    "        sum_tot_tot=w[0][:,0][j]*a[:,j]\n",
    "        sum_da=0\n",
    "        for k in range(x_test.shape[1]):\n",
    "            topk=get_top_k(np.abs(w[0][:,0][k]*np.matmul(np.expand_dims(x_test[:,k],axis=-1).T,grad_val[k,:,:])),top_num)\n",
    "            if j in topk:\n",
    "                sum_tot+=w[0][:,0][k]*grad_val[k,:,j]*x_test[:,k]\n",
    "            sum_tot_tot+=w[0][:,0][k]*grad_val[k,:,j]*x_test[:,k]\n",
    "            sum_da+=w[0][:,0][k]*grad_val[k,:,j]*x_test[:,k]\n",
    "        derivA.append(np.mean(np.abs(sum_tot*sigmoid_d(np.sum(w[0][:,0]*h,axis=1)))))\n",
    "        derivA_tot.append(np.mean(np.abs(sum_tot_tot*sigmoid_d(np.sum(w[0][:,0]*h,axis=1)))))\n",
    "        derivA_alone.append(np.mean(np.abs(sum_da)))\n",
    "    #Get the ranking\n",
    "    kax_loc=np.mean(np.abs(w[0][:,0]*h),axis=0)\n",
    "    ka_loc=np.mean(np.abs(w[0][:,0]*a),axis=0)\n",
    "    ax_loc=np.mean(np.abs(h),axis=0)\n",
    "    a_loc=np.mean(np.abs(a),axis=0)\n",
    "    dp_dx_loc=np.mean(np.abs(derivative),axis=0)\n",
    "    a_da_loc=derivA\n",
    "    a_da_tot_loc=derivA_tot\n",
    "    ranks_grad.append(compute_kendall(dp_dx_loc,true))\n",
    "    print(\"Grad:\",ranks_grad[i-1])\n",
    "    ranks_kax.append(compute_kendall(kax_loc,true))\n",
    "    print(\"KAX:\",ranks_kax[i-1])\n",
    "    ranks_ka.append(compute_kendall(ka_loc,true))\n",
    "    print(\"KA:\",ranks_ka[i-1])\n",
    "    ranks_a_ka.append(compute_kendall(a_da_loc,true))\n",
    "    print(\"K(Ai+DAi/DXi*Xi):\",ranks_a_ka[i-1])\n",
    "    ranks_a_ka_tot.append(compute_kendall(a_da_tot_loc,true))\n",
    "    print(\"K(Ai+sum(DAj/DXi*Xj)) :\",ranks_a_ka_tot[i-1])\n",
    "    ranks_da.append(compute_kendall(derivA_alone,true))\n",
    "    print(\"K(sum(DAj/DXi*Xj)) :\",ranks_da[i-1])\n",
    "    ranks_ax.append(compute_kendall(ax_loc,true))\n",
    "    print(\"AX:\",ranks_ax[i-1])\n",
    "    ranks_a.append(compute_kendall(a_loc,true))\n",
    "    print(\"A:\",ranks_a[i-1])\n",
    "    \n",
    "    #LIME\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(x_train , verbose=False,class_names=[str(x) for x in range(1)],discretize_continuous=True)\n",
    "    pick=submodular_pick.SubmodularPick(explainer, x_test, predict_fn, num_features=10, num_exps_desired=1)\n",
    "    lime_loc=[]\n",
    "    for l in list(pick.explanations[0].local_exp.values())[0]:\n",
    "        lime_loc.append(l[0])\n",
    "    \n",
    "    \n",
    "    with DeepExplain(session=tf.keras.backend.get_session()) as de:\n",
    "        # Need to reconstruct the graph in DeepExplain context, using the same weights.\n",
    "        input_tensors = model.inputs[0]\n",
    "        fModel = tf.keras.Model(model.inputs,model.output)\n",
    "        target_tensor = fModel(input_tensors)\n",
    "        dl = de.explain('deeplift', target_tensor, input_tensors,x_test)\n",
    "        sa=de.explain('saliency', target_tensor, input_tensors,x_test)\n",
    "        gi=de.explain('grad*input', target_tensor, input_tensors,x_test)\n",
    "    dl_loc=np.mean(np.abs(dl),axis=0)\n",
    "    sa_loc=np.mean(np.abs(sa),axis=0)\n",
    "    gi_loc=np.mean(np.abs(gi),axis=0)\n",
    "    ranks_dl.append(compute_kendall(dl_loc,true))\n",
    "    print(\"Deep Lift:\",ranks_dl[i-1])\n",
    "    ranks_sa.append(compute_kendall(sa_loc,true))\n",
    "    print(\"Saliency:\",ranks_sa[i-1])\n",
    "    ranks_gi.append(compute_kendall(gi_loc,true))\n",
    "    print(\"Grad*Input:\",ranks_gi[i-1])\n",
    "    ranks_LIME.append(compute_kendall(lime_loc,true))\n",
    "    print(\"LIME:\",ranks_LIME[i-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additive multi product\n",
    "results_circular=[]\n",
    "n=6\n",
    "#results_circular.append(print_results(ranks_grad,n,\"dp_dx\"))\n",
    "results_circular.append(print_results(ranks_kax,n,\"KAX on AM\"))\n",
    "results_circular.append(print_results(ranks_ka,n,\"KA on AM\"))\n",
    "results_circular.append(print_results(ranks_a_ka,n,\"KiAi+Kj*dAj/dXi*Xj {j \\ j in topk} on AM\"))\n",
    "results_circular.append(print_results(ranks_a_ka_tot,n,\"Ki*Ai+Kj*dAj/dXi*Xj {j=0 to N features} on AM\"))\n",
    "results_circular.append(print_results(ranks_da,n,\"Kj*dAj/dXi*Xj {j=0 to N features} on AM\"))\n",
    "results_circular.append(print_results(ranks_ax,n,\"AX on AM\"))\n",
    "results_circular.append(print_results(ranks_a,n,\"A on AM\"))\n",
    "results_circular.append(print_results(ranks_dl,n,\"Deep Lift on AM\"))\n",
    "results_circular.append(print_results(ranks_sa,n,\"Saliency on AM\"))\n",
    "results_circular.append(print_results(ranks_gi,n,\"Grad*Input on AM\"))\n",
    "results_circular.append(print_results(ranks_LIME,n,\"LIME on AM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "ranks_dlc=[]\n",
    "ranks_sac=[]\n",
    "ranks_gic=[]\n",
    "ranks_dp_dxc=[]\n",
    "ranks_L2X=[]\n",
    "ranks_LIMEc=[]\n",
    "model_layers=(9,5,10)\n",
    "epoch=100\n",
    "for train_index, val_index in indexes:\n",
    "    print(\"Model:\"+str(i))\n",
    "    x_train, x_val = x[train_index], x[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    tf.reset_default_graph()\n",
    "    #Model\n",
    "    inputs=tf.keras.Input(shape=(x_train.shape[1],))\n",
    "    inputs=tf.keras.Input(shape=(x.shape[1],))\n",
    "    network=FeedForward(num_feature=x.shape[1],list_reduc=model_layers,classification=True)\n",
    "    p,logit,dp_dx=network(inputs)\n",
    "    model=tf.keras.Model(inputs,p)\n",
    "    #losses\n",
    "    def loss_classic(y_true, y_pred):\n",
    "        loss_loc=tf.keras.losses.MSE(y_true, y_pred)\n",
    "        return loss_loc\n",
    "\n",
    "    def loss_classic_classification(y_true, y_pred):\n",
    "        loss_loc=tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "        return loss_loc\n",
    "    filepath=\"./feed_forward_non_linear_with_product2\"+str(i)+\".hdf5\"\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',verbose=0, save_best_only=True, mode='min',save_weights_only=True)\n",
    "    csv=tf.keras.callbacks.CSVLogger('./feed_forward_non_linear_with_product2'+str(i)+'.log')\n",
    "    i+=1\n",
    "    callbacks_list = [checkpoint,csv]\n",
    "    adam = tf.keras.optimizers.Adam(lr = 0.01)\n",
    "    model.compile(loss=loss_classic_classification,optimizer=adam,metrics=[loss_classic_classification])\n",
    "    \n",
    "    #Train Model\n",
    "    print(\"Training...\")\n",
    "    model.fit(x_train, y_train, validation_data=(x_val, y_val),callbacks=callbacks_list, epochs=epoch, batch_size=512,verbose=False)\n",
    "    \n",
    "    #get the best model back\n",
    "    model.load_weights(filepath)\n",
    "    print(\"Training done!\")\n",
    "    model.evaluate(x_test, y_test)\n",
    "    \n",
    "    #output the desired values\n",
    "    model5=tf.keras.Model(inputs,dp_dx)\n",
    "    derivative=model5.predict(x_test)\n",
    "    \n",
    "    #Get the ranking\n",
    "    with DeepExplain(session=tf.keras.backend.get_session()) as de:\n",
    "        # Need to reconstruct the graph in DeepExplain context, using the same weights.\n",
    "        input_tensors = model.inputs[0]\n",
    "        fModel = tf.keras.Model(model.inputs,model.output)\n",
    "        target_tensor = fModel(input_tensors)\n",
    "        dl = de.explain('deeplift', target_tensor, input_tensors,x_test)\n",
    "        sa=de.explain('saliency', target_tensor, input_tensors,x_test)\n",
    "        gi=de.explain('grad*input', target_tensor, input_tensors,x_test)\n",
    "    dl_loc=np.mean(np.abs(dl),axis=0)\n",
    "    sa_loc=np.mean(np.abs(sa),axis=0)\n",
    "    gi_loc=np.mean(np.abs(gi),axis=0)\n",
    "    dp_dx_loc=np.mean(np.abs(derivative),axis=0)\n",
    "    \n",
    "    #L2X\n",
    "    print(\"L2X\")\n",
    "    _,L2X_loc=call_L2X([x_train,np.array([1-y_train[:,0],y_train[:,0]]).T,\n",
    "              x_val,np.array([1-y_val[:,0],y_val[:,0]]).T,\n",
    "              x_test,np.array([1-y_test[:,0],y_test[:,0]]).T,\n",
    "              \"additive_non_linear\"])\n",
    "    \n",
    "    #LIME\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(x_train , verbose=False,class_names=[str(x) for x in range(1)],discretize_continuous=True)\n",
    "    pick=submodular_pick.SubmodularPick(explainer, x_test, predict_fn, num_features=10, num_exps_desired=1)\n",
    "    lime_loc=[]\n",
    "    for l in list(pick.explanations[0].local_exp.values())[0]:\n",
    "        lime_loc.append(l[0])\n",
    "    ranks_dlc.append(compute_kendall(dl_loc,true))\n",
    "    print(\"Deep Lift:\",ranks_dlc[i-1])\n",
    "    ranks_sac.append(compute_kendall(sa_loc,true))\n",
    "    print(\"Saliency:\",ranks_sac[i-1])\n",
    "    ranks_gic.append(compute_kendall(gi_loc,true))\n",
    "    print(\"Grad*Input:\",ranks_gic[i-1])\n",
    "    ranks_L2X.append(compute_kendall(L2X_loc,true))\n",
    "    print(\"L2X:\",ranks_L2X[i-1])\n",
    "    ranks_LIMEc.append(compute_kendall(lime_loc,true))\n",
    "    print(\"LIME:\",ranks_LIMEc[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_circular.append(print_results(ranks_dlc,n,\"Deep Lift on FF\"))\n",
    "results_circular.append(print_results(ranks_sac,n,\"Saliency on FF\"))\n",
    "results_circular.append(print_results(ranks_gic,n,\"Grad*Input on FF\"))\n",
    "results_circular.append(print_results(ranks_L2X,n,\"L2X on FF\"))\n",
    "results_circular.append(print_results(ranks_LIMEc,n,\"LIME on FF\"))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
